# Spark Map-Reduce: Querying and Optimization

> **Course Project** for **M111 - Big Data Management**, Master's in Data Science & Artificial Intelligence (DSIT)

---

## 📌 Overview

This project explores different join implementations and optimizations using **Apache Spark** on **Hadoop clusters**. It compares performance across multiple APIs (RDDs and DataFrames) and algorithms to determine the most efficient approach.

The project is divided into several subtasks, each found in its own subdirectory. It also includes performance evaluations and a report summarizing the results.

---

## 🛠️ Technologies Used

- Python
- Apache Spark (RDD / DataFrame APIs)
- PySpark
- Hadoop HDFS

> ⚙️ For setup instructions, refer to:
> - [Apache Spark Documentation](https://spark.apache.org/)
> - [Spark Downloads](https://spark.apache.org/downloads.html)
> - [Hadoop Single Node Cluster Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)

---

## 🚀 Getting Started

> **Note**: This project assumes you already have **Hadoop** and **Spark** installed and configured.

To begin working with the project:

1. Read the [`project_2023.pdf`](./project_2023.pdf) for an overview of the assignment and requirements.
2. Review the [`Report.pdf`](./Report.pdf), which documents all tasks, implementations, and results.
3. Navigate to the [`src/`](./src) directory to explore the code for each subtask.
4. Check the [`output/`](./output) directory for the results produced by each implementation.

---

## 📁 Repository Structure

├── src/ # Source code for all subtasks 
├── output.zip/ # Result files generated by the code, packaged code base 
├── project_2023.pdf # Task description and project outline 
├── Report.pdf # Detailed report and evaluation 
└── README.md # This file


